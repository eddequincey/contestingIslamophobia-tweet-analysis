{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### Title: Twitter_Handler_Filtered.py\n",
    "\n",
    "### Description: This program extracts various searches and fields\n",
    "### from twitter data and compiles them into csv files.\n",
    "\n",
    "# !pip install pandas\n",
    "# !pip install time\n",
    "# !pip install datetime\n",
    "# !pip install numpy\n",
    "# !pip install nltk\n",
    "# !pip install pickle\n",
    "\n",
    "### Extras to jupyter/datascience-notebook ???\n",
    "# Comment out as necessary\n",
    "# !pip install iso-639\n",
    "# !pip install xlsxwriter\n",
    "# !pip install wordcloud\n",
    "# !pip install emoji==1.7.0\n",
    "# !pip install regex\n",
    "\n",
    "### Used for top_followers // memory efficient\n",
    "import heapq\n",
    "### json - Parsing the json file as an object - makes filtering easier\n",
    "import json\n",
    "### numpy - data processing functionality\n",
    "import numpy as np\n",
    "### os - used for directory and processing\n",
    "import os\n",
    "### cPickle - used to upload the classifier object for bot ID\n",
    "import pickle\n",
    "### datetime - processing date time objects\n",
    "from datetime import datetime\n",
    "###\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from lib import tools\n",
    "from _write import *\n",
    "from _return import *\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "with open('../nltk_data/pstv.txt', 'r') as fd:\n",
    "    positive_words = fd.read().splitlines()\n",
    "\n",
    "with open('../nltk_data/ngtv.txt', 'r') as fd:\n",
    "    negative_words = fd.read().splitlines()\n",
    "\n",
    "def add_user_to_dict(analytics, user_id, name, screen_name, verified):\n",
    "    user_info = analytics['user_info']\n",
    "    if user_id not in user_info:\n",
    "        user_info[user_id] = {\n",
    "            'user': user_id,\n",
    "            'name': name,\n",
    "            'screen_name': screen_name,\n",
    "            'verified': verified,\n",
    "            'link_to_profile': 'https://twitter.com/' + screen_name\n",
    "        }\n",
    "\n",
    "def insert_retweet_analytics(rt_id, rt_text, user, analytics):\n",
    "    analytics['tweet_retweet_frequencies']['retweet_count'].update( [rt_id] )\n",
    "    analytics['tweet_retweet_frequencies']['retweet_user_count'].update( [user['id_str']] )\n",
    "    ### if text is not in retweet text list\n",
    "    if rt_id not in analytics['tweet_retweet_frequencies']['retweet_text']:\n",
    "        analytics['tweet_retweet_frequencies']['retweet_text'][rt_id] = rt_text\n",
    "        analytics['tweet_retweet_frequencies']['tweet_to_user_mapping'][rt_id] = user['id_str']\n",
    "        add_user_to_dict(analytics, user['id_str'], user['name'], user['screen_name'], user['verified'])\n",
    "\n",
    "def insert_quote_analytics(qt_id, analytics):\n",
    "    analytics['tweet_quote_frequencies']['quote_count'].update( [qt_id] )\n",
    "\n",
    "def insert_edge(source_id, source_screen_name, target_screen_name, group, analytics_edges):\n",
    "    obj = {\n",
    "        'source' : source_screen_name,\n",
    "        'target': target_screen_name,\n",
    "        'group': group,\n",
    "        'link': 'https://twitter.com/i/web/status/' + str(source_id)\n",
    "    }\n",
    "    analytics['network']['vals'].update( [target_screen_name] )\n",
    "    analytics['network']['edges'].append( obj )\n",
    "\n",
    "def insert_node(screen_name, user_id, analytics):\n",
    "    if user_id == 'N/A': return None\n",
    "    obj = {\n",
    "        'id': int(user_id),\n",
    "        'screen_name': str(screen_name),\n",
    "        'val': 1,\n",
    "        'link_to_profile': 'https://twitter.com/' + screen_name\n",
    "    }\n",
    "    analytics['network']['nodes'][screen_name] = obj\n",
    "\n",
    "def populate_prerequisite_data(tweet_data):\n",
    "    ### define tweet type\n",
    "    original_tweet, retweet, quote_retweet = return_tweet_type(tweet_data)\n",
    "\n",
    "    primary_text = return_tweet_text(tweet_data, retweet)\n",
    "    hashtags = return_hashtag_text(tweet_data, retweet)\n",
    "    secondary_text = ''\n",
    "    main_text = primary_text\n",
    "    analysis_text = primary_text\n",
    "\n",
    "    ### add to text if there is additional information\n",
    "    if quote_retweet:\n",
    "        analysis_text, main_text, secondary_text, quoted_hashtags = return_quote_data(tweet_data, primary_text)\n",
    "        if quoted_hashtags:\n",
    "            hashtags = hashtags + quoted_hashtags\n",
    "    \n",
    "    return original_tweet, retweet, quote_retweet, analysis_text, main_text, secondary_text, primary_text, hashtags\n",
    "\n",
    "def populate_object(obj, i, main_text, analysis_text, hashtags):\n",
    "    user = i['user']\n",
    "    ms = int(i['timestamp_ms'])\n",
    "    \n",
    "    ### if has geo data\n",
    "    if i['place'] is not None:\n",
    "        location = str(i['place']['name'])\n",
    "    else:\n",
    "        location = user['location']\n",
    "\n",
    "    location = location if location is not None else 'N/A'\n",
    "\n",
    "    try:\n",
    "        ratio_to_followers = user[\"followers_count\"] / (user[\"followers_count\"] + user[\"friends_count\"])\n",
    "    except ZeroDivisionError:\n",
    "        ratio_to_followers = 0\n",
    "\n",
    "    if bot_prediction:\n",
    "        features_for_bot_predict = {\n",
    "            'followers_count': user['followers_count'],\n",
    "            'friends_count': user['friends_count'],\n",
    "            'listed_count': user['listed_count'],\n",
    "            'statuses_count': user['statuses_count'],\n",
    "            'screenname_length': len(user['screen_name']),\n",
    "            'digits_in_screenname': sum(c.isdigit() for c in user['screen_name']),\n",
    "            'ratio_followers': ratio_to_followers\n",
    "        }\n",
    "\n",
    "        features = [np.array(list(features_for_bot_predict.values())).astype(float)]\n",
    "\n",
    "    obj['user_id'] = user['id_str'] \n",
    "    obj['tweet_id'] = i['id_str']\n",
    "    obj['name'] = user['name']\n",
    "    obj['screen name'] = user['screen_name']\n",
    "    obj['timestamp'] = datetime.fromtimestamp(ms/1000.0).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    obj['text'] = main_text\n",
    "    obj['bio'] = user['description']\n",
    "    obj['hashtags'] = hashtags\n",
    "    obj['link to tweet'] = tools.return_tweet_link(obj['tweet_id'])\n",
    "    obj['location'] = location\n",
    "    obj['follower count'] = user['followers_count']\n",
    "    obj['friends count'] = user['friends_count']\n",
    "    obj['listed count'] = user['listed_count']\n",
    "    obj['language_twitter'] = i['lang']\n",
    "    obj['status count'] = user['statuses_count']\n",
    "    obj['collocations_text'] = return_collocation_list(analysis_text, search_terms)\n",
    "    obj['collocations_bio'] = return_collocation_list(obj['bio'], positive_words + negative_words)\n",
    "    obj['verified'] = user['verified']\n",
    "    obj['bot_prediction'] = clf.predict_proba(features)[:,1] if bot_prediction else 0\n",
    "\n",
    "def twitter_parser_new(file, analytics, dataframe):\n",
    "    with open(file, 'rb') as input:\n",
    "        json_file = json.load(input)\n",
    "        ### for every json object in the json\n",
    "        for i in json_file:\n",
    "\n",
    "            ### if not a valid tweet break\n",
    "            if (len(i) <= 1): break\n",
    "\n",
    "            ### -------------------------------\n",
    "            ### -------- Prerequisite ---------\n",
    "            ### -------------------------------\n",
    "\n",
    "            ##### create empty dict\n",
    "            obj = tools.return_empty_JSON()\n",
    "            original_tweet, retweet, quote_retweet, analysis_text, main_text, secondary_text, primary_text, hashtags = populate_prerequisite_data(i)\n",
    "\n",
    "            ### -------------------------------\n",
    "            ### --------- Update JSON ---------\n",
    "            ### -------------------------------\n",
    "\n",
    "            populate_object(obj, i, main_text, analysis_text, hashtags)\n",
    "            dataframe.append(obj)\n",
    "\n",
    "            ### -------------------------------\n",
    "            ### -------- Preprocessing --------\n",
    "            ### -------------------------------\n",
    "\n",
    "            tweet_frequencies = return_clean_text(analysis_text)\n",
    "            tweet_emoji_frequencies = return_emoji_list(analysis_text)\n",
    "            bio_frequencies = return_clean_text(obj['bio'])\n",
    "            bio_emoji_frequencies = return_emoji_list(obj['bio'])\n",
    "            name_emoji_frequencies = return_emoji_list(i['user']['name'])\n",
    "            collocations_text_without_search_term = [[i[0], i[2]] for i in obj['collocations_text']]\n",
    "            collocations_bio_without_search_term = [[i[0], i[2]] for i in obj['collocations_bio']]\n",
    "\n",
    "            ### -------------------------------\n",
    "            ### ------ Update Analytics -------\n",
    "            ### -------------------------------\n",
    "\n",
    "            analytics['counter']['words'] += len(return_clean_text(analysis_text, False))\n",
    "            analytics['counter']['tweets'] += 1\n",
    "            analytics['hashtag_frequencies'].update( obj['hashtags'] )\n",
    "            analytics['location_frequencies'].update( [obj['location']] )\n",
    "            analytics['language_frequencies'].update( [obj['language_twitter']] )\n",
    "            analytics['timeline_frequencies'].append(int(i['timestamp_ms'])) ### TO-DO: take away refence to i\n",
    "            analytics['coloc_text_frequency']['contextual'].update(obj['collocations_text'])\n",
    "            analytics['coloc_bio_frequency']['contextual'].update(obj['collocations_bio'])\n",
    "            analytics['coloc_text_frequency']['unique'].update(np.unique(collocations_text_without_search_term))\n",
    "            analytics['coloc_bio_frequency']['unique'].update(np.unique(collocations_bio_without_search_term))\n",
    "            analytics['emoji_frequency']['text_total'].update(tweet_emoji_frequencies)\n",
    "            analytics['emoji_frequency']['text_unique'].update(np.unique(tweet_emoji_frequencies))\n",
    "            analytics['emoji_frequency']['bio_total'].update(bio_emoji_frequencies)\n",
    "            analytics['emoji_frequency']['bio_unique'].update(np.unique(bio_emoji_frequencies))\n",
    "            analytics['emoji_frequency']['name_total'].update(name_emoji_frequencies)\n",
    "            analytics['emoji_frequency']['name_unique'].update(np.unique(name_emoji_frequencies))\n",
    "            analytics['word_frequency']['total'].update(tweet_frequencies)\n",
    "            analytics['word_frequency']['unique'].update(np.unique(tweet_frequencies))\n",
    "            analytics['bio_word_frequency'].update(np.unique(bio_frequencies))\n",
    "\n",
    "            ### add users to hashap to look up at later analysis stages\n",
    "            add_user_to_dict(analytics, obj['user_id'], obj['name'], obj['screen name'], obj['verified'])\n",
    "\n",
    "            ### check if Tweet contains video or photos\n",
    "            if 'media' in i['entities']:\n",
    "                analytics['counter']['with_media'] += 1\n",
    "                for media in i['entities']['media']:\n",
    "                    analytics['media']['link'][media['media_url_https']] = media['expanded_url']\n",
    "                    analytics['media']['count'].update( [media['media_url_https']] )\n",
    "\n",
    "            ### does the tweet contain a URL\n",
    "            if 'urls' in i['entities']:\n",
    "                if len(i['entities']['urls']) != 0:\n",
    "                    analytics['counter']['with_url'] += 1\n",
    "                    for url in i['entities']['urls']:\n",
    "                        ### avoid other Twitter replies URLs\n",
    "                        if 'twitter.com' not in url['expanded_url']:\n",
    "                            analytics['URLS']['unqiue'].update([url['expanded_url']])\n",
    "                            analytics['URLS']['domain'].update([urlparse(url['expanded_url']).netloc])\n",
    "\n",
    "            ### select node generation\n",
    "            insert_node(i['user']['screen_name'], i['user']['id_str'], analytics)\n",
    "            if retweet:\n",
    "                analytics['counter']['retweets'] += 1\n",
    "                insert_node(i['retweeted_status']['user']['screen_name'],  i['retweeted_status']['user']['id_str'], analytics)\n",
    "                insert_edge(i['retweeted_status']['id'], i['user']['screen_name'], i['retweeted_status']['user']['screen_name'], 'retweet', analytics)\n",
    "                insert_retweet_analytics(i['retweeted_status']['id'], obj['text'], i['retweeted_status']['user'], analytics)\n",
    "            elif quote_retweet:\n",
    "                analytics['counter']['quote_retweets'] += 1\n",
    "                # insert_quote_node(i['id'], obj['user_id'], analytics)\n",
    "                # ### difference between how API 1.1 and 2.0 handles quotes\n",
    "                qt_screen_name = main_text.split()[1][1:-1]\n",
    "                try:\n",
    "                    quoted_id = i['quoted_status']['id']\n",
    "                except KeyError: \n",
    "                    quoted_id = i['quoted_status_id']\n",
    "                if qt_screen_name != 'N/A' or qt_screen_name != 'undefined':\n",
    "                    ### TO-DO: Figure out how quote users IDs can be added here\n",
    "                    insert_node(qt_screen_name,  0, analytics)\n",
    "                    insert_edge(i['id'], i['user']['screen_name'], qt_screen_name, 'quote', analytics)\n",
    "                ### TO-DO: Figure out how quote users can be added here\n",
    "                insert_quote_analytics(quoted_id, analytics)\n",
    "\n",
    "            ### increment count\n",
    "            ### 'N/A' is reserved for tweets that had no user data \n",
    "            if obj['user_id'] != 'N/A':\n",
    "                analytics['user_frequencies']['user_unique_tweet_count'].update( [ obj['user_id'] ] )\n",
    "            \n",
    "            ### add all original tweets to a timeline\n",
    "            if original_tweet:\n",
    "                analytics['counter']['original_tweets'] += 1\n",
    "                for times in priority_tweets_in_timestamps:\n",
    "                    if times[0] <= int(i['timestamp_ms']) < times[1]:\n",
    "                        analytics['priority_tweets_from_timestamps'].append\n",
    "                        (\n",
    "                            {\n",
    "                                'tweet id': obj['tweet_id'],\n",
    "                                'text': obj['text'],\n",
    "                                'user_id': obj['user_id']\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "            ### top users via the amount of followers they have using a heap\n",
    "            ### the heap has a drawback that is may miss a few followers \n",
    "            ### gained / losted during data collection\n",
    "            top_users_via_followers = analytics['user_frequencies']['top_users_via_followers']\n",
    "            if len(top_users_via_followers) < top_users_size:\n",
    "                if return_duplicate_check(top_users_via_followers, obj['user_id']) == False:\n",
    "                    heapq.heappush(top_users_via_followers, (obj['follower count'], obj['user_id']))\n",
    "            else:\n",
    "                smallest_follower_count = heapq.nsmallest(1, top_users_via_followers)[-1][0]\n",
    "                if smallest_follower_count < obj['follower count']:\n",
    "                    if return_duplicate_check(top_users_via_followers, obj['user_id']) == False:\n",
    "                        heapq.heappushpop(top_users_via_followers, (obj['follower count'], obj['user_id']))\n",
    "\n",
    "            ### -------------------------------\n",
    "            ### --------- Empty Buffer --------\n",
    "            ### -------------------------------\n",
    "\n",
    "            ### if buffer is beyond buffer_size; write to file\n",
    "            if (len(dataframe) >= buffer_size):\n",
    "                write_to_file(dataframe, to_dir, save_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/3_analysis/_write.py:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(to_dir + file, header=0, index_col=0, parse_dates=True)\n",
      "/home/jovyan/3_analysis/_write.py:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(to_dir + file, header=0, index_col=0, parse_dates=True)\n",
      "/home/jovyan/3_analysis/_write.py:482: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(to_dir + file, header=0, index_col=0, parse_dates=True)\n"
     ]
    }
   ],
   "source": [
    "    all_files = [\n",
    "        '../analysis_config/event/example.json',\n",
    "    ]\n",
    "\n",
    "    for file in all_files:\n",
    "        ### all user varibles that require changing for each run\n",
    "        variables = json.load(open(file))\n",
    "        ### -------------------------------------\n",
    "        ###\n",
    "        from_dir = variables['from_dir']\n",
    "        ###\n",
    "        to_dir = variables['to_dir']\n",
    "        ###\n",
    "        save_file_name = variables['save_file_name']\n",
    "        ### the amount of CSV rows files each file output\n",
    "        buffer_size = variables['buffer_size']\n",
    "        ### the amount of retweets to write to file\n",
    "        retweet_size = variables['retweet_size']\n",
    "        ### \n",
    "        top_users_size = variables['top_users_size']\n",
    "        ###\n",
    "        bot_prediction = variables['bot_prediction']\n",
    "        ### in milliseconds\n",
    "        priority_tweets_in_timestamps = variables['priority_tweets_in_timestamps']\n",
    "        ###\n",
    "        search_terms = variables['search_terms']\n",
    "        ### -------------------------------------\n",
    "\n",
    "        ### do not recommend this feature in the current state\n",
    "        if bot_prediction:\n",
    "            with open('clf.pickle', 'rb') as f:\n",
    "                clf = pickle.load(f)\n",
    "\n",
    "        analytics = tools.return_empty_analytics()\n",
    "        dataframe = []\n",
    "\n",
    "        json_files = [from_dir + pos_json for pos_json in os.listdir(from_dir) if pos_json.endswith('.json')]\n",
    "        num_of_file = len(json_files)\n",
    "\n",
    "        for count, f in enumerate(json_files):\n",
    "            # try:\n",
    "            # print('{}/{} - {}'.format(count+1, num_of_file, f))\n",
    "            twitter_parser_new(f, analytics, dataframe)\n",
    "            # except ValueError:  # includes simplejson.decoder.JSONDecodeError:\n",
    "            #     print ('JSON decode error')\n",
    "            #     pass\n",
    "        \n",
    "        write_to_file(dataframe, to_dir, save_file_name)\n",
    "        write_analytics_files(analytics, to_dir, retweet_size, top_users_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
